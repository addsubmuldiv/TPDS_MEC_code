# 网络模型设计

Twisted采用单线程模型，reactor，所有的IO操作、编码解码、业务逻辑等都是在一个线程中执行。



我们的需求是，

客户端：

- 能够按照泊松分布发送请求
- 接受到响应数据后做个统计，包括==请求的总时间==，==被哪个服务器执行的==

服务端

- 作为转发服务器$J$来说，监听客户端的请求$S$，并自己作为客户端将请求$S$发给执行服务器$K$，然后收到$K$的返回数据以后，再把数据返回给自己连的那个客户端
- 作为执行服务器$K$来说，接收到请求$S$，执行一些操作，将结果返回转发服务器$J$

## 客户端

1. 一个客户端和一个服务器绑定，这个客户端**只和这个服务器收发数据**，所以客户端不需要知道其他服务器的信息，服务器也不需要知道其他客户端的信息。
2. 客户端需要同时能够收发数据，而发数据为了能够持续不断、且服从泊松分布，需要sleep，因此要有单独的一个线程来处理。
3. 客户端接收数据可以直接用主线程来处理，在日志或者数据库中，记录一个请求的起止时间



## 服务器

### 作为转发服务器

 	1. 作为接入服务器，它是一个服务端程序，主要工作是**接收请求**，然后根据一个**路由算法**，选择执行服务器，然后起一个**新线程**，自己继续接收请求（这里可以用线程池）
 	2. 在这个**新线程**中，这个服务器作为客户端，去连接执行服务器，收到执行服务器返回的数据后，再把这个数据发回给客户端，也就是说，执行和转发要解耦

### 作为执行服务器

 	1. 执行服务器和转发服务器要分成两个程序来写，执行服务程序就只管来了请求，执行一个耗时算法，把结果返回即可，因此会比较简单。。





**总体上，要写三个程序**

m个服务，n个服务器

1. 客户端程序
   1. 发起m个计时线程专门算延时，根据泊松分布，到点了就起一个线程（线程池实现）去发送请求，这个线程就处理后续的统计工作，计时线程继续算延时
2. 转发服务器程序
   1. 主线程就持续监听，如果发现有请求到来，就开一个工作线程去处理
   2. 工作线程的工作是，解析请求，给请求选一个执行服务器，然后作为客户端给执行服务器发送数据，等到计算完，数据回来了，再返回给客户端（线程池实现）
3. 执行服务器程序
   1. 主线程监听请求，如果有请求到来，分配工作线程
   2. 工作线程进行一些耗时运算，处理完直接返回


**为了方便处理，数据发送的时候先发个byte长度，然后再发byte数据**

### 6-6

目前完成了单个线程情况下，客户端到服务端的正常通信，服务端返回的暂未测试。。
其实也还没完成

预计明天
1. 看看水平触发导致的，在目前写法下（即主线程IO复用，从线程负责数据搬运，以及后续处理），多个线程频繁响应一个数据的问题是否还有
   - 看起来是不太行，水平触发还是水平触发，开新线程处理缓冲区搬运的时候，就是会频繁响应
2. 解决客户端延时异常的问题，调试从服务器发到客户端能否正常
   - 延时异常解决了
   - 服务端发回给客户端正常了
3. 测试客户端多个线程并发是否正常，，这里可能存在一些OOM之类的，缓冲区开大了的问题
   - 并发测试通过
   - 但是存在一个问题，就是不好确认这个阻塞队列的长度，任务新增的速度容易大于处理的速度，以至于抛异常
   - 上面说的这个问题，其实不是这个问题，因为debug的时候发现线程池里面的工作线程一直是2，没有到最大线程数量，主要是客户端那边不知道为啥，自己停在read那里，就死等，导致任务积压，通过在两端调用shutdownOutput解决了。
4. 开始着手写执行服务器的代码


### 6-7
7号完成的工作，都在6号里面写了

### 6-8
写执行服务器的代码，考虑到对于不同服务，分配资源不同，这个东西，岂不是要把各服务分配不同的线程池？

针对不同服务需要额外添加的模块有
1. 客户端，需要m个延时程序，对于不同服务的数据量要做些控制有所区别，线程池公用即可。。。或者也可以把lambda设大点，然后中间通过摇号决定选哪个服务？这样编程简单点。。
2. 转发服务器，不需要考虑，只需知道是哪个服务的请求，然后就转发就完事
3. 执行服务器，需要知道是哪个服务，然后执行不同复杂度的操作，那么不同服务需要不同线程池吗？

1. 最终采用了开多个线程来算延时的方案，感觉比随机摇号靠谱，顺便用了一手CountDownLatch
2. ok的，没毛病
3. 最终采取多个线程池的方案，这样才能确保哪个服务分配资源多，哪个服务资源少


### 6-10
整个实验的方法论应该是怎样的？
首先明确我们的目的是，验证理论算的和实际跑的差不多。。
而不是在实际场景下去做对比实验，因为我们无法细粒度地去分配资源的多少。而理论计算的几个对比方法，可能就差那一点，而实际中无法操作

1. 我们的几个服务让它们先纯跑，看看一段时间内，在给定的这些线程的情况下，每一台服务器单位时间能处理多少个，以此作为mu/w的值
2. 然后加上既有的网络参数，设置到我们的数值计算模型里面，去计算一个结果出来
3. 实际去跑一下，看看得到的结果和算出来的差多少，差的不多就行
4. 以上所说的，目前大概应该是指期望值


基于以上的考虑，下一步的计划是，写一下各个客户端的数据统计模块，目前来看主要要
1. 统计各个服务的请求完成数量，请求完成时间
2. 基于1，计算各个服务的请求时间平均值
   将这些记录在文件里。
   

每个服务的workload设置成一样，这样子每台机器对每个服务的处理能力也就是一样的，其他就是网速的区别

那是不是也可以直接让输入数据和输出数据和实际执行的操作直接分离开，我随便跑个耗时的运算就完事了，都不需要实际对输入输出数据做运算

3个客户端，3个服务端
2个服务

N0 客户端工作线程数
N1 接入服务器程序工作线程数
N21,N22 执行服务器工作线程数
客户端总共：3 * (1 + 2 + N0)
服务端总共：3 * (1 + N1) + 3 * (1 + N21 + N22)



6-20新方案-总体思路
1. 3台虚拟机，各自装docker

2. 在docker里面部署我们的执行服务器应用，配置其cpu资源（命令行/dockerfile）

3. 通过route命令或者网桥，配置这些不同宿主机上docker可以互相访问

4. 转发程序可以直接放在宿主机上

5. 虚拟机配置网速限制

6. 客户端程序可以直接放在物理机上，分别请求不同的虚拟机即可，方便记录结果

   


6-20-step1
1. 装好docker，分配相应资源，部署一个简单程序，测试其处理能力
2. 在执行服务器程序里添加日志记录，确保出现问题好追踪
3. 打包执行服务器程序、放到docker里面，写dockerfile，让它启动就运行起来



6-21-step2

- 执行服务器
  1. 规定，执行服务器程序对外暴露端口8899
  2. 对于docker端口映射，在程序中监听8899，然后docker 用 -p 映射到虚拟机端口，然后服务器id通过dockerfile设置到程序里面
  3. 使用 --cpus=x 会比较好，x在里面线程池总数为8的时候用4以下比较合适，大概就是线程池数量的一半左右，这时候是线性增长的，即线程数和总处理时间

- 转发服务器
  1. 转发服务器端口 8988
  2. 转发服务器程序直接放在虚拟机上来执行，事先要把各个执行服务器的IP设置进去
- 客户端
  1. 客户端直接放在物理机上执行，分别启动3个
  2. 3个实例可以用jar直接调命令一次执行，也可以直接在idea里面启动3个（idea不好控制同时发起，不过大概一点点误差也无所谓？）


6-27
开始逐步实现上述内容，进行测试
- 为了简单起见，采用两个服务，3个服务器，即 m=2，n=3
- 访问容器就用端口映射至虚拟机，实际访问就是访问虚拟机的IP+映射的端口
- 在转发服务器保存一个列表，即各个虚拟机的IP地址，这样才能访问执行服务器进行决策

6-28
- 初步测试，发现执行服务器和转发服务器之间的数据传输出现问题，有空指针异常，需要先把这两个之间调通
    - 因此，先在本地机器上，把三个程序运行起来，调试至能正常处理一些请求
    
- 整合测试通过，期间解决了如下问题：
    1. 转发服务器和执行服务器之间数据传输有误，没有发送数据长度，导致错误解析，引发OOM
    2. 转发服务器发送数据的时候，控制语句的<写成<=，导致死循环的问题
    3. 执行服务器的NIO的attachment因为为了避免水平触发，在重新注册关掉监听的时候把它置为了null，导致获取attachment的时候导致空指针的问题
    

7-4
1. 根据公式P1，设计一组参数，主要是请求到达率lambda，要控制好，不太大太小
2. 设置好请求到达率，在有限个逻辑CPU的环境下（目前来看CPU小于4都能够复合线性增长的需求，即CPU加倍，时间减半），测试mu/w的量，即gamma
3. 算出gamma，我们假定所有theta都一样，在这个基础上直接套公式（这里似乎甚至不需要去调求解器，因为我们的决策变量都定下来了），算出理论平均请求延时A
4. 将网络带宽配置到虚拟机，然后实际运行一下，统计所有请求的实际平均请求延时B
5. 只要A和B差不太远，就OK了

7-5
cpu==1，80s左右
cpu==2，38s左右，平均1.41s
cpu==3，20s左右，平均0.77s
cpu==4，16s左右，平均0.56s
以上不用了。。
====原来的测试方式不对，要测试CPU速度，直接把任务丢进去就完事了，这才是它真实水平，不需要去延时，它反而造成干扰====

目前来看请求到达率不太需要专门去设计。。

最坏的情况是，队列一直积压，处理不过来了，但这样有助于全速运作。。。。

cpu=2，200个任务，100*1000复杂度，2.7s   gamma=75  平均每个0.1s
cpu=4，200个任务，100*1000复杂度，1.3s   gamma=150  平均每个0.05s

参数设计
2个service，3个server
2MB的数据，传一趟假定0.5s，网速就是 4 MB/s == 4096 KB/s == 32768 Kbps
每个请求的时间，大约2s多点

7-6
在将各个项目部署至实验环境时，出现了bug，客户端发数据给接入服务器，似乎会出现不发，或者一直阻塞在那里的情况

目前想到的解决方案是，之前图省事没用缓冲区，一次将2MB发了，将其改成用缓冲区来发试试。。

再不行，就把接入服务器部署到Windows的虚拟机上

以上依旧不行。。。。

记录一下目前的问题

1. 程序是可以超出时限后停止的，test_0.txt文件有被更新
2. 自动跑会卡在某个地方，似乎是网络阻塞了，但是在单机又没毛病
3. 点击调试的时候可以一直跑到主程序结束，但是整个程序不退出，而单机上可以退出
4. 在不退出的情况下可以看到有些请求发了一半，但是就停在那里了，整个流程似乎也不推进
5. 发送数据阻塞是个什么情况？

想了一个下策：
在windows虚拟机里面跑client和access，网络传输延迟手动加上。。
只有执行服务器是正常用的

验证了一下，确实client和access在不同机器上的时候，就不对；在一块就还ok


7-7 
！！！大坑！！！
1. 彻底解决了昨天遇到的问题，即远程环境下无法正常发送和接收数据
2. 主要在于，有时候读的太快，发的速度跟不上，会导致提前退出while循环，因此在读的时候，while里面要记录读了多少数据，是否够了
3. 还有就是有时候是发的太快，接收的跟不上，发送缓冲区还没移动，新数据又来了，如果不做处理，会导致还没发送的发送缓冲区被新数据覆盖掉，需要做一些处理

以上2、3两点都会导致收发数据不全的问题，不仅仅是flush就能解决的。



7-12
又改了改bug，完善了一下代码。。
然后真正做了一下实验，实际跑1.3s，但是理论计算只有1.1s，可能是线程调度耗时？